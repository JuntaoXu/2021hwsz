{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JuntaoXu/2021hwsz/blob/main/mobilenetv2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "! pip install torchmetrics"
      ],
      "metadata": {
        "id": "aWB5UjxoDKZb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os, glob, random\n",
        "random.seed(42)\n",
        "\n",
        "def save2txt(path, savepath='labels.txt'):\n",
        "    print(os.path.exists(path))\n",
        "    with open(savepath, 'w') as t:\n",
        "        imgfiles = glob.iglob(os.path.join(path, '**/*.tif'), recursive=True)\n",
        "        for imgfile in imgfiles:\n",
        "            imgname = os.path.split(imgfile)[-1]\n",
        "            label = 0 if 'OK' in imgfile else 1\n",
        "            # label = 0 if imgfile.split('/')[-2] == 'OK' else 1\n",
        "            t.write(imgname + '\\t' + str(label) + '\\n')\n",
        "        t.close()\n",
        "\n",
        "def split_data(txtpath):\n",
        "    with open(txtpath, 'r') as t:\n",
        "        train_data, val_data, test_data = [], [], []\n",
        "        lines = t.readlines()\n",
        "        random.shuffle(lines)\n",
        "        for line in lines:\n",
        "            p = random.randint(0,9)\n",
        "            if p == 0: # 8\n",
        "                val_data.append(line)\n",
        "            elif p == 1: # 9\n",
        "                test_data.append(line)\n",
        "            else:\n",
        "                train_data.append(line)\n",
        "        savedir = os.path.split(txtpath)[0]\n",
        "        with open(os.path.join(savedir, 'traindata1.txt'), 'w') as traintxt:\n",
        "            for data in train_data:\n",
        "                traintxt.write(data)\n",
        "            traintxt.close()\n",
        "        with open(os.path.join(savedir, 'valdata0.txt'), 'w') as valtxt:\n",
        "            for data in val_data:\n",
        "                valtxt.write(data)\n",
        "            valtxt.close()\n",
        "        with open(os.path.join(savedir, 'testdata1.txt'), 'w') as testtxt:\n",
        "            for data in test_data:\n",
        "                testtxt.write(data)\n",
        "            testtxt.close()\n",
        "        t.close()\n",
        "\n",
        "\n",
        "path = '/content/drive/MyDrive/yema/yema_dataset_bright0215/'\n",
        "savepath = os.path.join(path, 'labels.txt')\n",
        "save2txt(path, savepath)\n",
        "split_data(savepath)"
      ],
      "metadata": {
        "id": "FQZ_wQi31W_O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-_st9C5bh2Ba"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torchmetrics.classification import BinaryAccuracy\n",
        "import os, cv2\n",
        "import numpy as np\n",
        "import gc\n",
        "from PIL import Image\n",
        "gc.collect()\n",
        "torch.cuda.empty_cache()\n",
        "\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Hyper-parameters\n",
        "num_epochs = 200\n",
        "\n",
        "# Image preprocessing modules\n",
        "transform = transforms.Compose([\n",
        "    # transforms.Pad(4),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.RandomVerticalFlip(),\n",
        "    transforms.RandomRotation((-12, 12)),\n",
        "    transforms.RandomAdjustSharpness(sharpness_factor=2),\n",
        "    # transforms.RandomCrop((480, 640), ),\n",
        "    transforms.GaussianBlur(kernel_size=(5, 9), sigma=(0.1, 1.5)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.45, 0.225)])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    # transforms.Pad(4),\n",
        "    # transforms.RandomHorizontalFlip(),\n",
        "    # transforms.RandomCrop(32),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(0.45, 0.225)])\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class myDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, labels_file, transform):\n",
        "        labels = {}\n",
        "        list_IDs = []\n",
        "        with open(labels_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                ID, label = line.rstrip('\\n').split('\\t')\n",
        "                labels[ID] = int(label)\n",
        "                list_IDs.append(ID)\n",
        "            f.close()\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        print(self.labels)\n",
        "        self.data_path = os.path.split(labels_file)[0]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "        \n",
        "        # Load data and get label\n",
        "        if self.labels[ID] == 0:\n",
        "            # imgpath = os.path.join(self.data_path, 'OK', ID)\n",
        "            if os.path.exists(os.path.join(self.data_path, 'OK', '1', ID)):\n",
        "                imgpath = os.path.join(self.data_path, 'OK', '1', ID)  \n",
        "            else:\n",
        "                imgpath = os.path.join(self.data_path, 'OK', '2', ID)\n",
        "\n",
        "        else:\n",
        "            imgpath = os.path.join(self.data_path, 'NG', ID)\n",
        "        X = Image.open(imgpath)\n",
        "        y = self.labels[ID]\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return X, y"
      ],
      "metadata": {
        "id": "sS5arvv1CFjD",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        },
        "outputId": "e53fc44f-56ff-41e1-f33a-f0ad76b09412"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-013b7680a488>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mmyDataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mlist_IDs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generators\n",
        "# Parameters\n",
        "params = {'batch_size': 20,\n",
        "          'shuffle': True,\n",
        "          'num_workers': 2}\n",
        "\n",
        "train_path = os.path.join(path, 'traindata1.txt')\n",
        "val_path = os.path.join(path, 'valdata0.txt')\n",
        "test_path = os.path.join(path, 'testdata1.txt')\n",
        "training_set = myDataset(train_path, transform)\n",
        "train_loader = torch.utils.data.DataLoader(training_set, **params)\n",
        "\n",
        "validation_set = myDataset(val_path, val_transform)\n",
        "val_loader = torch.utils.data.DataLoader(validation_set, **params)\n",
        "\n",
        "test_set = myDataset(test_path, val_transform)\n",
        "test_loader = torch.utils.data.DataLoader(test_set, **params)\n",
        "\n",
        "save_model = '/content/drive/MyDrive/yema/models/mv2_0215/'\n",
        "if not os.path.exists(save_model):\n",
        "    os.makedirs(save_model)"
      ],
      "metadata": {
        "id": "tQfYEjxiCH4e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "__all__ = ['MobileNetV2', 'mobilenet_v2']\n",
        "\n",
        "\n",
        "model_urls = {\n",
        "    'mobilenet_v2': 'https://download.pytorch.org/models/mobilenet_v2-b0353104.pth',\n",
        "}\n",
        "\n",
        "\n",
        "def _make_divisible(v, divisor, min_value=None):\n",
        "    \"\"\"\n",
        "    This function is taken from the original tf repo.\n",
        "    It ensures that all layers have a channel number that is divisible by 8\n",
        "    It can be seen here:\n",
        "    https://github.com/tensorflow/models/blob/master/research/slim/nets/mobilenet/mobilenet.py\n",
        "    :param v:\n",
        "    :param divisor:\n",
        "    :param min_value:\n",
        "    :return:\n",
        "    \"\"\"\n",
        "    if min_value is None:\n",
        "        min_value = divisor\n",
        "    new_v = max(min_value, int(v + divisor / 2) // divisor * divisor)\n",
        "    # Make sure that round down does not go down by more than 10%.\n",
        "    if new_v < 0.9 * v:\n",
        "        new_v += divisor\n",
        "    return new_v\n",
        "\n",
        "class ConvBNReLU(nn.Sequential):\n",
        "    def __init__(self, in_planes, out_planes, kernel_size=3, stride=1, groups=1):\n",
        "        padding = (kernel_size - 1) // 2\n",
        "        super(ConvBNReLU, self).__init__(\n",
        "            nn.Conv2d(in_planes, out_planes, kernel_size, stride, padding, groups=groups, bias=False),\n",
        "            nn.BatchNorm2d(out_planes),\n",
        "            nn.ReLU6(inplace=True)\n",
        "        )\n",
        "\n",
        "class InvertedResidual(nn.Module):\n",
        "    def __init__(self, inp, oup, stride, expand_ratio):\n",
        "        super(InvertedResidual, self).__init__()\n",
        "        self.stride = stride\n",
        "        assert stride in [1, 2]\n",
        "\n",
        "        hidden_dim = int(round(inp * expand_ratio))\n",
        "        self.use_res_connect = self.stride == 1 and inp == oup\n",
        "\n",
        "        layers = []\n",
        "        if expand_ratio != 1:\n",
        "            # pw\n",
        "            layers.append(ConvBNReLU(inp, hidden_dim, kernel_size=1))\n",
        "        layers.extend([\n",
        "            # dw\n",
        "            ConvBNReLU(hidden_dim, hidden_dim, stride=stride, groups=hidden_dim),\n",
        "            # pw-linear\n",
        "            nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n",
        "            nn.BatchNorm2d(oup),\n",
        "        ])\n",
        "        self.conv = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.use_res_connect:\n",
        "            return x + self.conv(x)\n",
        "        else:\n",
        "            return self.conv(x)\n",
        "\n",
        "\n",
        "class MobileNetV2(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_classes=2,\n",
        "                 width_mult=0.8,\n",
        "                 inverted_residual_setting=None,\n",
        "                 round_nearest=8,\n",
        "                 block=None):\n",
        "        \"\"\"\n",
        "        MobileNet V2 main class\n",
        "        Args:\n",
        "            num_classes (int): Number of classes\n",
        "            width_mult (float): Width multiplier - adjusts number of channels in each layer by this amount\n",
        "            inverted_residual_setting: Network structure\n",
        "            round_nearest (int): Round the number of channels in each layer to be a multiple of this number\n",
        "            Set to 1 to turn off rounding\n",
        "            block: Module specifying inverted residual building block for mobilenet\n",
        "        \"\"\"\n",
        "        super(MobileNetV2, self).__init__()\n",
        "\n",
        "        if block is None:\n",
        "            block = InvertedResidual\n",
        "        input_channel = 32\n",
        "        last_channel = 1280\n",
        "\n",
        "\n",
        "        if inverted_residual_setting is None:\n",
        "            inverted_residual_setting = [\n",
        "                # t, c, n, s\n",
        "                [1, 16, 1, 1],\n",
        "                [6, 24, 2, 2],\n",
        "                [6, 32, 3, 2],\n",
        "                [6, 64, 4, 2],\n",
        "                [6, 96, 3, 1],\n",
        "                [6, 160, 3, 2],\n",
        "                [6, 320, 1, 1],\n",
        "            ]\n",
        "\n",
        "        # only check the first element, assuming user knows t,c,n,s are required\n",
        "        if len(inverted_residual_setting) == 0 or len(inverted_residual_setting[0]) != 4:\n",
        "            raise ValueError(\"inverted_residual_setting should be non-empty \"\n",
        "                             \"or a 4-element list, got {}\".format(inverted_residual_setting))\n",
        "\n",
        "        # building first layer\n",
        "        input_channel = _make_divisible(input_channel * width_mult, round_nearest)\n",
        "        self.last_channel = _make_divisible(last_channel * max(1.0, width_mult), round_nearest)\n",
        "        features = [ConvBNReLU(1, input_channel, stride=2)] # modified from 3 to 1 for grayscale input\n",
        "        # building inverted residual blocks\n",
        "        for t, c, n, s in inverted_residual_setting:\n",
        "            output_channel = _make_divisible(c * width_mult, round_nearest)\n",
        "            for i in range(n):\n",
        "                stride = s if i == 0 else 1\n",
        "                features.append(block(input_channel, output_channel, stride, expand_ratio=t))\n",
        "                input_channel = output_channel\n",
        "        # building last several layers\n",
        "        features.append(ConvBNReLU(input_channel, self.last_channel, kernel_size=1))\n",
        "        # make it nn.Sequential\n",
        "        self.features = nn.Sequential(*features)\n",
        "\n",
        "        # building classifier\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(self.last_channel, num_classes),\n",
        "        )\n",
        "\n",
        "        # weight initialization\n",
        "        for m in self.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_out')\n",
        "                if m.bias is not None:\n",
        "                    nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.BatchNorm2d):\n",
        "                nn.init.ones_(m.weight)\n",
        "                nn.init.zeros_(m.bias)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\n",
        "                nn.init.zeros_(m.bias)\n",
        "\n",
        "    def _forward_impl(self, x):\n",
        "        # This exists since TorchScript doesn't support inheritance, so the superclass method\n",
        "        # (this one) needs to have a name other than `forward` that can be accessed in a subclass\n",
        "        x = self.features(x)\n",
        "        x = x.mean([2, 3])\n",
        "        x = self.classifier(x)\n",
        "        return x\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self._forward_impl(x)\n",
        "\n",
        "def mobilenet_v2(pretrained=False, progress=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Constructs a MobileNetV2 architecture from\n",
        "    `\"MobileNetV2: Inverted Residuals and Linear Bottlenecks\" `_.\n",
        "    Args:\n",
        "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
        "        progress (bool): If True, displays a progress bar of the download to stderr\n",
        "    \"\"\"\n",
        "    model = MobileNetV2(**kwargs)\n",
        "    if pretrained:\n",
        "        state_dict = load_state_dict_from_url(model_urls['mobilenet_v2'],\n",
        "                                              progress=progress)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model\n",
        "\n",
        "model = mobilenet_v2().to(device)\n",
        "\n"
      ],
      "metadata": {
        "id": "to2rxrk_EtWw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\n",
        "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=40, eta_min=0.00001)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "\n",
        "# best epoch\n",
        "best_acc = 0\n",
        "best_model = model\n",
        "best_epoch = 0\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        optimizer.zero_grad()\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        \n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        \n",
        "        # Backward and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        \n",
        "        if (i+1) % 40 == 0:\n",
        "            print (\"Epoch [{}/{}], Step [{}/{}] Loss: {:.4f}\"\n",
        "                   .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
        "        \n",
        "    if epoch % 5 == 0:\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            correct = 0\n",
        "            total = 0\n",
        "            \n",
        "            for images, labels in val_loader:\n",
        "                images = images.to(device)\n",
        "                labels = labels.to(device)\n",
        "                outputs = model(images)\n",
        "                # _, predicted = torch.max(outputs.data, 1)\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                # print(predicted)\n",
        "                total += labels.size(0)\n",
        "                # print(predicted == labels)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                # y_pred = outputs.cpu().detach().numpy()[:, 1]\n",
        "                # print(y_pred)\n",
        "                # y_pred = (y_pred >= 0.5).astype(int)\n",
        "                # print(y_pred)\n",
        "                # total += labels.size(0)\n",
        "                # labels = np.array(labels)\n",
        "                # correct += (y_pred == labels).astype(int).sum().item()\n",
        "            acc = correct / total\n",
        "            if acc > best_acc:\n",
        "                best_acc = acc\n",
        "                best_epoch = epoch\n",
        "                best_model = model\n",
        "                torch.save(model.state_dict(), save_model + 'best.ckpt')\n",
        "\n",
        "            print('Accuracy of the epoch {} on the val images: {} %'.format(epoch, 100 * correct / total))\n",
        "            print('Best accuracy is {} %, from epoch {}'.format(100 * best_acc, best_epoch))\n",
        "\n",
        "        # Save the model checkpoint\n",
        "        # torch.save(model.state_dict(), save_model + 'epoch{}_mv2.ckpt'.format(epoch))\n",
        "\n",
        "    # Update learning rate\n",
        "    scheduler.step()\n",
        "    \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HYriPE-iHJvA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "best_model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = best_model(images)\n",
        "                \n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # print(predicted)\n",
        "        total += labels.size(0)\n",
        "        # print(predicted == labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the best model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(best_model.state_dict(), save_model + 'best_mv2.ckpt')\n",
        "\n",
        "\n",
        "model.eval()\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        # print(predicted)\n",
        "        total += labels.size(0)\n",
        "        # print(predicted == labels)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the last model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "# Save the model checkpoint\n",
        "torch.save(model.state_dict(), save_model + 'last_mv2.ckpt')"
      ],
      "metadata": {
        "id": "96x1EqNTPqPJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "289fcb0f-5924-4ab3-f191-1f1d8a36e7ba"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the best model on the test images: 97.87798408488064 %\n",
            "Accuracy of the last model on the test images: 97.87798408488064 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Load model and test"
      ],
      "metadata": {
        "id": "NH3tVlAxnXwC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "\n",
        "\n",
        "class testDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, labels_file, transform):\n",
        "        labels = {}\n",
        "        list_IDs = []\n",
        "        with open(labels_file, 'r') as f:\n",
        "            lines = f.readlines()\n",
        "            for line in lines:\n",
        "                ID, label = line.rstrip('\\n').split()\n",
        "                labels[ID] = int(label)\n",
        "                list_IDs.append(ID)\n",
        "            f.close()\n",
        "        self.labels = labels\n",
        "        self.list_IDs = list_IDs\n",
        "        print(self.labels)\n",
        "        self.data_path = os.path.split(labels_file)[0]\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.list_IDs)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        'Generates one sample of data'\n",
        "        # Select sample\n",
        "        ID = self.list_IDs[index]\n",
        "        \n",
        "        # Load data and get label\n",
        "        if self.labels[ID] == 0:\n",
        "            # imgpath = os.path.join(self.data_path, 'OK', ID)\n",
        "            if os.path.exists(os.path.join(self.data_path, 'OK', '1', ID)):\n",
        "                imgpath = os.path.join(self.data_path, 'OK', '1', ID)  \n",
        "            else:\n",
        "                imgpath = os.path.join(self.data_path, 'OK', '2', ID)\n",
        "        else:\n",
        "            imgpath = os.path.join(self.data_path, 'NG', ID)\n",
        "        X = Image.open(imgpath)\n",
        "        y = self.labels[ID]\n",
        "        if self.transform:\n",
        "            X = self.transform(X)\n",
        "\n",
        "        return imgpath, X, y\n",
        "\n",
        "def testModel(model_path, test_labels, save_path):\n",
        "    if not os.path.exists(save_path):\n",
        "        os.makedirs(save_path)\n",
        "    test_set = testDataset(test_labels, val_transform)\n",
        "    test_loader = torch.utils.data.DataLoader(test_set, **params)\n",
        "\n",
        "    model = mobilenet_v2().to(device)\n",
        "    model.load_state_dict(torch.load(model_path))\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        fn, fp = 0, 0\n",
        "        count0, count1 = 0, 0\n",
        "        for img_paths, images, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            labels = labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            preds = outputs.data.cpu()\n",
        "            # print(predicted)\n",
        "            total += labels.size(0)\n",
        "            # print(predicted == labels)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            for idx, d in enumerate(preds):\n",
        "                out = np.exp(d)\n",
        "                out = out/sum(out)\n",
        "                pred_label = 1 if out[0] < 0.9 else 0\n",
        "                label = labels[idx].item()\n",
        "                if label == 0:\n",
        "                    count0 += 1\n",
        "                else:\n",
        "                    count1 += 1\n",
        "                if label == 0 and label != pred_label:\n",
        "                    fn += 1\n",
        "                    \n",
        "                    print(out)\n",
        "                elif label == 1 and label != pred_label:\n",
        "                    print(img_paths[idx])\n",
        "                    print(out)\n",
        "                    \n",
        "                    fp += 1\n",
        "            # for idx, item in enumerate(torch.eq(predicted, labels)):\n",
        "            #     if not item:\n",
        "            #         label = labels[idx].item()\n",
        "            #         print(img_paths[idx])\n",
        "            #         print(label)\n",
        "            #         out = np.exp(preds[idx])\n",
        "            #         out = out / sum(out)\n",
        "            #         print(out)\n",
        "            #         save_img = os.path.join(save_path, str(label))\n",
        "            #         if not os.path.exists(save_img):\n",
        "            #             os.makedirs(save_img)\n",
        "            #         shutil.copy(img_paths[idx], save_img)\n",
        "                    \n",
        "        print(fn, fp)\n",
        "        print(count0, count1)\n",
        "        # print('Accuracy of the last model on the test images: {} %'.format(100 * correct / total))\n",
        "\n",
        "model_path = '/content/drive/MyDrive/yema/models/mv2_0215/best_mv2.ckpt'\n",
        "save_imgs = '/content/drive/MyDrive/yema/test_results/mv2_0215/'\n",
        "testModel(model_path, test_path, save_imgs)\n"
      ],
      "metadata": {
        "id": "bT7D32AgncbJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}